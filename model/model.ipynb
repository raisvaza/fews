{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead time: berapa jarak forecast-nya?\n",
    "# X = [wl(t-5), wl(t-4),wl(t-3), wl(t-2), wl(t-1), rf(t-5), rf(t-4), rf(t-3), rf(t-2), rf(t-1)], y = wl(t)\n",
    "\n",
    "# VARIABLE SELECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nash-Sutcliffe Efficiency NEED TO BE VERIFIED\n",
    "def nse(y_true, y_pred):\n",
    "    numerator = tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred)))\n",
    "    denominator = tf.reduce_sum(tf.square(tf.subtract(y_true, tf.reduce_mean(y_true))))\n",
    "    \n",
    "    nse = 1 - (numerator / denominator)\n",
    "    \n",
    "    return nse\n",
    "\n",
    "# Kling-Gupta Efficiency NEED TO BE VERIFIED\n",
    "def kge(y_true, y_pred):\n",
    "    # Calculate r (correlation coefficient)\n",
    "    r = tf.reduce_sum((y_true - tf.reduce_mean(y_true)) * (y_pred - tf.reduce_mean(y_pred))\n",
    "                      / (tf.sqrt(tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true))) * tf.reduce_sum(tf.square(y_pred - tf.reduce_mean(y_pred))))))\n",
    "\n",
    "    # Calculate alpha (standard deviation ratio)\n",
    "    alpha = tf.sqrt(tf.reduce_sum(tf.square(y_pred - tf.reduce_mean(y_pred))) / tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true))))\n",
    "\n",
    "    # Calculate beta (mean ratio)\n",
    "    beta = tf.reduce_mean(y_pred) / tf.reduce_mean(y_true)\n",
    "\n",
    "    # Calculate KGE\n",
    "    kge = 1 - tf.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "    return kge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 15ms/step - loss: 157.3672 - val_loss: 157.0043\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 157.1651 - val_loss: 156.8067\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 156.9486 - val_loss: 156.5884\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 156.7054 - val_loss: 156.3405\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 156.4191 - val_loss: 156.0461\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 156.0779 - val_loss: 155.6944\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 155.6689 - val_loss: 155.2671\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 155.1797 - val_loss: 154.7614\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 154.5899 - val_loss: 154.1465\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 153.8950 - val_loss: 153.4213\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 153.0874 - val_loss: 152.5821\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 152.1463 - val_loss: 151.6382\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 151.0903 - val_loss: 150.5449\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 149.8772 - val_loss: 149.3281\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 148.5315 - val_loss: 147.9578\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 147.0101 - val_loss: 146.4616\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 145.3632 - val_loss: 144.7938\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 143.5450 - val_loss: 142.9991\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 141.5900 - val_loss: 141.0567\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 139.4865 - val_loss: 139.0032\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 137.2720 - val_loss: 136.7867\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 134.9038 - val_loss: 134.5024\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 132.4402 - val_loss: 132.0456\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 129.8424 - val_loss: 129.4752\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 127.0961 - val_loss: 126.8679\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 124.2987 - val_loss: 124.0591\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 121.3548 - val_loss: 121.1720\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 118.2885 - val_loss: 118.2001\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 115.1319 - val_loss: 115.1693\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 111.9383 - val_loss: 111.9725\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 108.5809 - val_loss: 108.7747\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 105.1882 - val_loss: 105.4333\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 101.6805 - val_loss: 102.0526\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 98.1104 - val_loss: 98.5200\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 94.4271 - val_loss: 94.9456\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 90.7058 - val_loss: 91.2351\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 86.8772 - val_loss: 87.5311\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 83.0085 - val_loss: 83.7252\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 79.0896 - val_loss: 79.7603\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 75.0728 - val_loss: 75.7330\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 70.9333 - val_loss: 71.6777\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 66.7944 - val_loss: 67.5586\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 62.5742 - val_loss: 63.3986\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 58.3843 - val_loss: 59.1522\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 54.0916 - val_loss: 54.9396\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 49.8385 - val_loss: 50.6210\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 45.5565 - val_loss: 46.4612\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 41.4366 - val_loss: 42.3381\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 37.5492 - val_loss: 38.4002\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 34.0970 - val_loss: 35.0484\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 31.3673 - val_loss: 32.6547\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 29.2537 - val_loss: 30.7864\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 27.4924 - val_loss: 29.3711\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 26.0433 - val_loss: 28.2420\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 24.8089 - val_loss: 27.2787\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 23.7846 - val_loss: 26.5403\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 22.9337 - val_loss: 25.8139\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 22.2629 - val_loss: 25.2618\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 21.7647 - val_loss: 24.8483\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 21.3124 - val_loss: 24.3898\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 20.9559 - val_loss: 24.1120\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 20.6499 - val_loss: 23.7803\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 20.4364 - val_loss: 23.5729\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 20.2495 - val_loss: 23.3012\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 20.0901 - val_loss: 23.1333\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.9247 - val_loss: 22.9243\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.8015 - val_loss: 22.7755\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.6477 - val_loss: 22.6204\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.5477 - val_loss: 22.4197\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.4578 - val_loss: 22.3648\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.3541 - val_loss: 22.3004\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.2697 - val_loss: 22.1825\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.2106 - val_loss: 22.0781\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.1057 - val_loss: 22.0312\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 19.0086 - val_loss: 21.9463\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.9327 - val_loss: 21.8501\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.8383 - val_loss: 21.7743\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.7748 - val_loss: 21.7069\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.7084 - val_loss: 21.6664\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.6263 - val_loss: 21.5418\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.5563 - val_loss: 21.4903\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.4729 - val_loss: 21.4281\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 18.4044 - val_loss: 21.4052\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 18.3269 - val_loss: 21.3336\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 18.2481 - val_loss: 21.2490\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 18.1767 - val_loss: 21.1911\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 18.1059 - val_loss: 21.1172\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 18.0059 - val_loss: 21.0060\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 17.9276 - val_loss: 20.9819\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.8557 - val_loss: 20.8890\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.7755 - val_loss: 20.7718\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.6786 - val_loss: 20.7249\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.5866 - val_loss: 20.6383\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.5164 - val_loss: 20.5606\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 17.4444 - val_loss: 20.4627\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.3529 - val_loss: 20.3484\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 17.2681 - val_loss: 20.2958\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 17.1857 - val_loss: 20.2122\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 17.0966 - val_loss: 20.1286\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 16.9951 - val_loss: 20.0593\n"
     ]
    }
   ],
   "source": [
    "# Generate a toy dataset for regression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a simple ANN model for regression\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(20,)),        # Input layer with 20 features\n",
    "    keras.layers.Dense(32, activation='relu'),  # Hidden layer with 32 neurons and ReLU activation\n",
    "    keras.layers.Dense(1)                    # Output layer for regression (no activation function)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some example data\n",
    "# Replace this with  actual dataset\n",
    "data = np.random.rand(100, 2)  # Simulated data with 2 features (rainfall and water level)\n",
    "\n",
    "# Split the data into input (X) and output (y)\n",
    "X = data[:-1]  # Input at time t (rainfall and water level)\n",
    "y = data[1:, 1]  # Output at time t+1 (water level)\n",
    "\n",
    "# Define RNN parameters\n",
    "input_dim = 2  # Number of input features (rainfall and water level)\n",
    "hidden_units = 64  # Number of RNN units\n",
    "output_dim = 1  # Number of output features (water level prediction)\n",
    "\n",
    "# Create the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=hidden_units, activation='tanh', input_shape=(None, input_dim), return_sequences=False),\n",
    "    tf.keras.layers.Dense(output_dim)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape the data to fit the RNN input shape\n",
    "X = X.reshape(-1, 1, input_dim)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "# Now you can use the trained model for predictions\n",
    "# For example, to predict water level at t+1 based on input at t\n",
    "input_t = np.array([[0.5, 0.6]])  # Replace with  input data\n",
    "input_t = input_t.reshape(1, 1, input_dim)\n",
    "prediction_t_plus_1 = model.predict(input_t)\n",
    "\n",
    "print(\"Predicted Water Level at t+1:\", prediction_t_plus_1[0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and preprocess  data as before\n",
    "\n",
    "# Define GRU parameters\n",
    "input_dim = 2  # Number of input features (rainfall and water level)\n",
    "hidden_units = 64  # Number of GRU units\n",
    "output_dim = 1  # Number of output features (water level prediction)\n",
    "\n",
    "# Create the GRU model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(units=hidden_units, activation='tanh', input_shape=(None, input_dim), return_sequences=False),\n",
    "    tf.keras.layers.Dense(output_dim)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape the data to fit the GRU input shape\n",
    "X = X.reshape(-1, 1, input_dim)\n",
    "\n",
    "# Train the model as before\n",
    "\n",
    "# To predict water level at t+1 based on input at t using the trained model\n",
    "input_t = np.array([[0.5, 0.6]])  # Replace with  input data\n",
    "input_t = input_t.reshape(1, 1, input_dim)\n",
    "prediction_t_plus_1 = model.predict(input_t)\n",
    "\n",
    "print(\"Predicted Water Level at t+1:\", prediction_t_plus_1[0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and preprocess data as before\n",
    "\n",
    "# Define LSTM parameters\n",
    "input_dim = 2  # Number of input features (rainfall and water level)\n",
    "hidden_units = 64  # Number of LSTM units\n",
    "output_dim = 1  # Number of output features (water level prediction)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=hidden_units, activation='tanh', input_shape=(None, input_dim), return_sequences=False),\n",
    "    tf.keras.layers.Dense(output_dim)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape the data to fit the LSTM input shape\n",
    "X = X.reshape(-1, 1, input_dim)\n",
    "\n",
    "# Train the model as before\n",
    "\n",
    "# To predict water level at t+1 based on input at t using the trained model\n",
    "input_t = np.array([[0.5, 0.6]])  # Replace with  input data\n",
    "input_t = input_t.reshape(1, 1, input_dim)\n",
    "prediction_t_plus_1 = model.predict(input_t)\n",
    "\n",
    "print(\"Predicted Water Level at t+1:\", prediction_t_plus_1[0, 0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
